{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee98ac0",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "The following libraries are imported for this notebook:\n",
    "\n",
    "- **PyTorch** (`torch`, `torch.nn`, `torch.utils.data`): For building and training neural networks.\n",
    "- **Torchvision** (`transforms`, `datasets.ImageFolder`): For image transformations and dataset handling.\n",
    "- **TensorBoard** (`torch.utils.tensorboard.SummaryWriter`): For logging and visualizing training metrics.\n",
    "- **Scikit-learn** (`accuracy_score`, `f1_score`, `precision_score`, `recall_score`, `confusion_matrix`): For evaluation metrics.\n",
    "- **Seaborn** and **Matplotlib**: For plotting and visualization.\n",
    "- **tqdm**: For progress bars during training.\n",
    "\n",
    "These imports enable data loading, preprocessing, model building, training, evaluation, and visualization throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1818579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d698c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = SummaryWriter(log_dir=\"/Users/arponbiswas/Computer-Vision-Projects/Image_classification_projects/PC_Parts_Image_Classification/Tensorboard_Graph/Models_Graph/SimpleModel/v_1.0\")\n",
    "writer = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507126f3",
   "metadata": {},
   "source": [
    "## Training Data Transformations\n",
    "\n",
    "The following transformations are applied to the training images:\n",
    "\n",
    "1. **Geometric Transformations:**\n",
    "    - `Resize((256, 256))`: Resizes all images to 256x256 pixels.\n",
    "    - `RandomHorizontalFlip(p=0.5)`: Randomly flips images horizontally with a 50% probability.\n",
    "    - `RandomRotation(10)`: Rotates images randomly within Â±10 degrees.\n",
    "    - `RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.9, 1.1))`: Applies small translations and scaling for positional and size variance.\n",
    "    - `RandomPerspective(distortion_scale=0.1, p=0.3, interpolation=3)`: Applies subtle 3D perspective distortions with a 30% probability.\n",
    "\n",
    "2. **Color Augmentations:**\n",
    "    - `ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05)`: Introduces mild variations in brightness, contrast, saturation, and hue.\n",
    "\n",
    "3. **Grayscale Conversion and Normalization:**\n",
    "    - `Grayscale(num_output_channels=1)`: Converts images to single-channel grayscale.\n",
    "    - `ToTensor()`: Converts images to PyTorch tensors.\n",
    "\n",
    "These augmentations help improve model robustness by simulating real-world variations in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b6b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trans = transforms.Compose([\n",
    "    # 1. Geometric Transformations (with resizing)\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Horizontal flips for left-right consistency\n",
    "    transforms.RandomRotation(10),  # Minor rotation for realistic variation\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,  # No additional rotation\n",
    "        translate=(0.05, 0.05),  # Small positional variance\n",
    "        scale=(0.9, 1.1)  # Conservative scaling for proportion preservation\n",
    "    ),\n",
    "    transforms.RandomPerspective(distortion_scale=0.1, p=0.3, interpolation=3),  # Subtle 3D perspective\n",
    "\n",
    "    # 2. Color Augmentations\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05\n",
    "    ),  # Mild color variation for natural lighting\n",
    "\n",
    "    # 3. Grayscale conversion, ToTensor, and Normalization for 1 channel\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to 1 channel\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952cc9a",
   "metadata": {},
   "source": [
    "## Dataset and Data Loading\n",
    "\n",
    "The dataset consists of images of PC parts, organized in subfolders by class. Images are loaded using PyTorch's `ImageFolder`, which automatically assigns labels based on folder names.\n",
    "\n",
    "For training, the dataset is loaded with the defined transformations to augment and preprocess the images. The `DataLoader` is used to efficiently batch and shuffle the data during training, enabling parallel data loading and improved training performance.\n",
    "\n",
    "- **Dataset root:** `/Users/arponbiswas/Computer-Vision-Projects/Image_classification_projects/PC_Parts_Image_Classification/Data/pc_parts_ready`\n",
    "- **Transformations:** See previous section for details on augmentation and preprocessing.\n",
    "- **Batch size:** 16\n",
    "- **Shuffling:** Enabled for training\n",
    "\n",
    "This setup ensures that the model receives diverse and well-preprocessed data for robust learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7f6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root='/Users/arponbiswas/Computer-Vision-Projects/Image_classification_projects/PC_Parts_Image_Classification/Data/pc_parts_ready', transform=train_trans)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9c953e",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "The neural network model is instantiated and prepared for training and evaluation in this section. The model object is created and can be used for forward passes, loss computation, and optimization. This step ensures the model is ready to receive input data and participate in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1c4aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.version = '1.0'\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=1, kernel_size=6, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1 * 21 * 21, 14)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2508ed",
   "metadata": {},
   "source": [
    "## Model Instantiation and TensorBoard Visualization\n",
    "\n",
    "In this section, the `SimpleCNN` model is instantiated and a sample image from the training dataset is prepared for visualization. The following steps are performed:\n",
    "\n",
    "- **Model Creation:**  \n",
    "    The `SimpleCNN` neural network is instantiated and assigned to the variable `model`. This model is designed for grayscale image classification with a simple convolutional architecture.\n",
    "\n",
    "- **Sample Image Preparation:**  \n",
    "    A single image is extracted from the training dataset (`train_dataset[8][0]`) and reshaped with `unsqueeze(0)` to add a batch dimension, making it compatible with the model's expected input shape.\n",
    "\n",
    "- **TensorBoard Graph Logging:**  \n",
    "    The model's computational graph is logged to TensorBoard using `writer.add_graph(model, img)`. This allows for visual inspection of the model architecture in TensorBoard.\n",
    "\n",
    "- **Feature Map Visualization:**  \n",
    "    The function `add_image_to_tensorboard` is defined to:\n",
    "    - Log the original input image to TensorBoard.\n",
    "    - Pass the image through each convolutional and pooling layer of the model.\n",
    "    - Log the resulting feature maps after each major operation, providing insight into how the model transforms the input at each stage.\n",
    "\n",
    "- **Function Execution:**  \n",
    "    The visualization function is called with the sample image and model, enabling detailed monitoring of feature extraction and transformation within the network.\n",
    "\n",
    "This setup aids in debugging, understanding model behavior, and ensuring that data flows correctly through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fac3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SimpleCNN model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Get a sample image from the training dataset and add a batch dimension\n",
    "img = train_dataset[8][0].unsqueeze(0)\n",
    "\n",
    "# Add the model graph to TensorBoard for visualization\n",
    "writer.add_graph(model, img)\n",
    "\n",
    "# Define a function to log images and feature maps to TensorBoard\n",
    "def add_image_to_tensorboard(trans_image, model):\n",
    "    # Log the original input image\n",
    "    writer.add_image(\"Actual_Image\", trans_image.squeeze(0))\n",
    "    # Iterate through model layers\n",
    "    for name, layers in model.named_children():\n",
    "        if isinstance(layers, nn.Sequential):\n",
    "            for layer in layers:\n",
    "                # If the layer is Conv2d, pass the image through and log the feature map\n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    trans_image = layer(trans_image)\n",
    "                    writer.add_image(\"Conv2d\", trans_image.squeeze(0)[:3])\n",
    "                # If the layer is MaxPool2d, pass the image through and log the feature map\n",
    "                if isinstance(layer, nn.MaxPool2d):\n",
    "                    trans_image = layer(trans_image)\n",
    "                    writer.add_image(\"MaxPool2d\", trans_image.squeeze(0)[:3])\n",
    "\n",
    "# Call the function to log the sample image and feature maps\n",
    "add_image_to_tensorboard(img, model)\n",
    "\n",
    "# Display the model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552f76d",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer\n",
    "\n",
    "In this section, the loss function and optimizer for training the neural network are defined:\n",
    "\n",
    "- **Loss Function:**  \n",
    "    `nn.CrossEntropyLoss()` is used as the loss criterion. This loss is suitable for multi-class classification problems, as it measures the difference between the predicted class probabilities and the true class labels.\n",
    "\n",
    "- **Optimizer:**  \n",
    "    The Adam optimizer (`torch.optim.Adam`) is initialized with the model's parameters and a learning rate of 0.001. Adam is an adaptive learning rate optimization algorithm that combines the advantages of two other extensions of stochastic gradient descent: AdaGrad and RMSProp. It is widely used for training deep learning models due to its efficiency and effectiveness.\n",
    "\n",
    "These components are essential for guiding the model's learning process during training by updating the model weights to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c03c472",
   "metadata": {},
   "source": [
    "## Training Loop and Performance Analysis\n",
    "\n",
    "This section defines the `training_analysis` function, which manages the end-to-end training process for the neural network and provides comprehensive performance monitoring:\n",
    "\n",
    "- **Device Selection:**  \n",
    "    Automatically detects and utilizes a GPU if available, otherwise defaults to CPU, ensuring efficient computation.\n",
    "\n",
    "- **Epoch-wise Training:**  \n",
    "    For each epoch:\n",
    "    - Sets the model to training mode.\n",
    "    - Iterates over the training data in batches, performing forward and backward passes.\n",
    "    - Computes the loss using the defined loss function and updates model weights via the optimizer.\n",
    "    - Tracks predictions and true labels for metric calculation.\n",
    "\n",
    "- **Progress Monitoring:**  \n",
    "    Utilizes `tqdm` to display a real-time progress bar with current loss, enhancing transparency during training.\n",
    "\n",
    "- **Metric Calculation:**  \n",
    "    After each epoch, calculates key performance metrics:\n",
    "    - **Accuracy**\n",
    "    - **F1 Score**\n",
    "    - **Precision**\n",
    "    - **Recall**\n",
    "    - **Confusion Matrix**\n",
    "\n",
    "- **TensorBoard Logging:**  \n",
    "    Logs loss, accuracy, F1 score, precision, recall, and the confusion matrix to TensorBoard, enabling detailed visualization and analysis of the training process.\n",
    "\n",
    "- **Confusion Matrix Visualization:**  \n",
    "    Plots the confusion matrix using Seaborn and Matplotlib, providing insights into class-wise prediction performance.\n",
    "\n",
    "- **Summary Output:**  \n",
    "    Prints a concise summary of metrics for each epoch, facilitating quick assessment of model progress.\n",
    "\n",
    "This function streamlines the training workflow and ensures that both quantitative and qualitative aspects of model performance are thoroughly tracked and visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_analysis(model, epochs=10):\n",
    "    # Select device: use GPU if available, else fallback to CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_loss = 0.0  # Accumulate loss for the epoch\n",
    "        all_preds = []    # Store all predictions for metric calculation\n",
    "        all_labels = []   # Store all true labels for metric calculation\n",
    "        \n",
    "        # Progress bar for batches in the current epoch\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{epochs}]\", leave=False)\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to device\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = loss_fn(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model parameters\n",
    "            epoch_loss += loss.item()  # Accumulate batch loss\n",
    "            all_preds.extend(outputs.argmax(dim=1).tolist())  # Store predicted classes\n",
    "            all_labels.extend(labels.tolist())  # Store true classes\n",
    "\n",
    "            # Update progress bar with current batch loss\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Calculate metrics for the epoch\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        # Log scalar metrics to TensorBoard\n",
    "        writer.add_scalar('Loss/train', epoch_loss / len(train_loader), epoch)\n",
    "        writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
    "        writer.add_scalar('F1/train', f1, epoch)\n",
    "        writer.add_scalar('Precision/train', precision, epoch)\n",
    "        writer.add_scalar('Recall/train', recall, epoch)\n",
    "\n",
    "        # Plot and log confusion matrix to TensorBoard\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "        ax.set_title('Confusion Matrix')\n",
    "        writer.add_figure('Confusion Matrix', fig, global_step=epoch)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Print summary of metrics for the epoch\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(train_loader):.4f} Accuracy: {accuracy:.4f} F1: {f1:.4f} Precision: {precision:.4f} Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf30178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_analysis(model, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
